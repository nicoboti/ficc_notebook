{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3710bfdb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../figuras/logos/logo_usc.jpg\" align=right width='80px'/>\n",
    "<br>\n",
    "\n",
    "\n",
    "<table width=\"100%\">\n",
    "<td style=\"font-size:40px;font-style:italic;text-align:right;background-color:rgba(0, 220, 170,0.7)\">\n",
    "Classical Information Theory\n",
    "</td></table>\n",
    "\n",
    "\n",
    "\n",
    "$ \\newcommand{\\bra}[1]{\\langle #1|} $\n",
    "$ \\newcommand{\\ket}[1]{|#1\\rangle} $\n",
    "$ \\newcommand{\\braket}[2]{\\langle #1|#2\\rangle} $\n",
    "$ \\newcommand{\\ketbra}[2]{| #1\\rangle \\langle #2|} $\n",
    "$ \\newcommand{\\tr}{{\\rm Tr}\\,} $\n",
    "$ \\newcommand{\\Tr}{{\\rm Tr}\\,} $\n",
    "$ \\newcommand{\\i}{{\\color{blue} i}} $ \n",
    "$ \\newcommand{\\Hil}{{\\cal H}} $\n",
    "$ \\newcommand{\\V}{{\\cal V}} $\n",
    "$ \\newcommand{\\Lin}{\\hbox{Lin}}$\n",
    "$ \\newcommand{\\Xn}{X^{\\! n}}$\n",
    "$ \\newcommand{\\xn}{{\\bf x}}$\n",
    "$ \\newcommand{\\bxn}{\\bar{\\bf x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3daa108",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<a id='top'></a>\n",
    "\n",
    "- [Elements of classical Information](#class_info)\n",
    "\n",
    "    - [Shannon Entropy](#shannon)\n",
    "\n",
    "    - [Data Compression](#data_com)\n",
    "    \n",
    "<br>\n",
    "\n",
    "- [Joint entropies](#joint_ent)\n",
    "\n",
    "    - [Conditional Entropy](#cond_ent)\n",
    "    \n",
    "    - [Mutual Information](#mut_ent)\n",
    "    \n",
    "    - [Relative Entropy](#rel_ent)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5608d032",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='class_info'></a>\n",
    "\n",
    "# Elements of classical information\n",
    "[<<<](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4099a90d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For completeness, let’s review the fundamental aspects of classical information theory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176cc4f1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "**Information** and **Probability** are dual concepts: \n",
    " a *random variable* $(X, p_X)$ produces events $x$ with probability $p_X(x)$.\n",
    "\n",
    "*information* of an event is proportional to the *uncertainty that it removes* \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a6f856",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- There is no uncertainty removal (no information gain) from a *sure event* $p(x)=1$.  \n",
    "\n",
    "- On the contrary, a *rare event* $p(x)\\ll 1$, increases our information a lot  \n",
    "\n",
    " \n",
    " \n",
    " can we quantify information, $i_X(x)$?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d9e6e2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Claude Shannon: the birth of information theory as a science. \n",
    "\n",
    "<img src=\"figures/Shannon 2.png\" width=\"15%\" style=\"margin:auto\"/>\n",
    "\n",
    "*A Mathematical Theory of Communication* Bell System Technical Journal, Vol. 27, pp. 379–423, 623–656, 1948 \n",
    "\n",
    "- 1st part: The source coding theorem (July 1948)\n",
    "\n",
    "- 2nd part: The noisy channel theorem (October 1948)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a4752e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Mathematical properties expected for, $i_X(x)$, must be: continuous, monotonous and additive. \n",
    "\n",
    "\n",
    " - $$i_{ X}(x) \\sim 1/p_X(x)$$\n",
    "<br>\n",
    "\n",
    " - $i_X(x \\land y) = i_X(x) + i_X( y)~$ for $x$ and $y$ independent\n",
    "<br>\n",
    "\n",
    " - $i_X(x) = 0~$ if $~p_X(x)=1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ff9619",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Shannon's information measure of an event  is the **surprise function**\n",
    "\n",
    "$$\n",
    "i_X(x) =  - \\log_2 p_X(x) \\,  ~ \\hbox{(bits)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9b1293",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='shannon'></a>\n",
    "## Shannon Entropy\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\", text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Definition:</b> <i>(Shannon Entropy)</i>  \n",
    "<br>\n",
    "The Shannon entropy associated with a classical random variable $X = (x, p(x))$ is given by the <i>average surprise</i>\n",
    "<br>\n",
    "<br>    \n",
    "$$\n",
    "H(X) = -\\sum_{x} p(x) \\log p_X(x) = -\\overline{\\log p_X}\n",
    "$$  \n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3901c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The (Shannon) entropy   measures  \n",
    "\n",
    "- the *average uncertainty* associated to the random variable\n",
    "<br>\n",
    "- the *average  information* that we gain by knowing events $x$ of $X$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d212cb77",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Hence $H(X)$ is an *average* over events $x$ in which \n",
    "\n",
    "- $x$ with  $p_X(x)\\approx 1$ barely contributes to $H(X)$ because it has little information, \n",
    "<br>\n",
    "- $x$ with $p_X(x)\\approx 0$ carries a lot of information but very rarely appears.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648ebe85",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Shannon entropy has the following properties:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\", text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Theorem:</b>  Let $N$ be the cardinality of  $X$, then    \n",
    "<br>\n",
    "<br>    \n",
    "$$0 \\leq H(X) \\leq \\log N$$\n",
    "<br>\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31842766",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<details>\n",
    "    <summary><p style=\"text-align:right ; color:black\"> >> <i>Proof</i> </p></summary>\n",
    "<br>\n",
    "\\begin{align}\n",
    " H(X) - \\log N &= \\sum_x p(x) \\log \\frac{1}{p(x)} - \\sum_x p(x) \\log N \\nonumber \\\\\n",
    "&=  \\sum_x p(x) \\log \\left(  \\frac{1}{N p(x)} \\right) \\leq  \\sum_x p(x)  \\left(  \\frac{1}{N p(x)}  - 1\\right) \\nonumber\\\\\n",
    "&=   \\sum_x  \\left(  \\frac{1}{N }  - p(x)\\right) = 0 ~~~\\Rightarrow ~~~~ \\fbox{$H(X) \\leq \\log N$} \\nonumber\n",
    "\\end{align}\n",
    "The inequality $\\log x \\leq x-1$ has been used and follows from plotting\n",
    "<img src=\"figures/logarithm.png\" width=\"30%\" style=\"margin:auto\"/>\n",
    "    \n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4385b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " The two limits correspond to particular *random variables*\n",
    " \n",
    " -  $H(X) =0$ $~~$ for the *certain* distribution (no removable uncertainty)\n",
    " \n",
    " $$p_X(x) = \\delta_{x,x_0}$$  \n",
    "\n",
    "<br>\n",
    "\n",
    " -  $H(X)= \\log N$ $~~$ for the  *even*  distribution (maximal removable average uncertainty)\n",
    " \n",
    "\n",
    "$$p_X(x) = \\frac{1}{N}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d0aa71",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Suppose $(X,p_X)$ and $(Y,p_Y)$ are two random variables over the same set $X = Y = \\{x_i\\}$.\n",
    "\n",
    "Then $(Z, p_Z)$ with $Z = X = Y$ and $p_Z = \\lambda p_X + (1-\\lambda) p_Y$ is a *bona fide* random variable. \n",
    "\n",
    "We abreviate this\n",
    "random variable by $Z = \\lambda X + (1-\\lambda) Y$ by abuse of language. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37d1dc3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\", text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Definition:</b>   \n",
    "$H(X)$ is a <i>concave</i> function of $X$. For $\\lambda \\in (0,1)$ and $X$, $Y$ two random variables\n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "H\\left( \\rule{0mm}{4mm} \\lambda X + (1-\\lambda) Y)\\right) ~ \\geq ~ \\lambda H(X) + (1-\\lambda) H(Y)\n",
    "$$\n",
    "</p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ced4a2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Concavity implies that there is **more average removable uncertainty** in $p_Z =  \\lambda p_X(x) + (1-\\lambda)p_Y(y)$, than  the **sum of removable uncertainties** in  each random variable individually.\n",
    "\n",
    "The [proof of concavity](#proof_of_concavity) will follow later from the positivity of the relative entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be7aa92",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\", text-align:center>\n",
    "<b>Example:</b>\n",
    "\n",
    "The entropy of the binary (or Bernoulli) distribution $ \\{ (a,b), (p, 1-p) \\} $ is  \n",
    "<br>\n",
    "$$\n",
    "H(p) = - p\\log p - (1-p)\\log (1-p)\n",
    "$$\n",
    "<br>\n",
    "We can see in the figure that it is indeed a concave function\n",
    "<br>\n",
    "<br>    \n",
    "    \n",
    "<img src=\"figures/binaryentropy.png\" width=\"30%\" style=\"margin:auto\"/>\n",
    "\n",
    "The case $p = 1/2$ marks the maximum of the entropy. At this value, on average, knowing the outcome $a$ or $b$ of a trial removes the greatest amount of uncertainty.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9673a877",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='data_comp'></a>\n",
    "## Data Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c123d137",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Another interpretation of $H(X)$ comes from the amount of resources needed to transmit messages composed of *letters* from the random variable $X$. This is the content of Shannon’s first theorem.\n",
    "\n",
    "To transmit a message through a channel, a process of compressed encoding is used. Typically, this is done in bits $\\{0,1\\}$, but using dits $\\{0,...,D-1\\}$ is also possible.\n",
    "\n",
    "In fact, associated with encoding in dits, we define $H_D(X) = -\\sum_{x} p(x) \\log_D p(x)$. When $D$ is not specified, we assume $D=2$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d850a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\", text-align:center>\n",
    "<b>Example:</b>\n",
    "encode an <i>alphabet</i> made of 4 letters\n",
    "$x=\\{A,B,C,D\\}$ which appear with probabilities $p_X(x)$ using bit-strings of length $l(x)$where\n",
    "<br>\n",
    "<br>\n",
    "    \n",
    "$$\n",
    "p(A) = \\frac{1}{2}~,~~ p(B)=\\frac{1}{4}~,~~ p(C) =p(D) = \\frac{1}{8}.\n",
    "$$\n",
    "<br>\n",
    "    \n",
    "- <u>strategy 1</u>. Encode uniformly with 2 bits for 4 letters \n",
    "\n",
    "$$\n",
    "A= 00~,~~ B=01~,~~ C=10~,~~ D=11 \\, .\n",
    "$$     \n",
    "The average <b>codeword length per letter</b> is\n",
    "    \n",
    "$$ \n",
    "L = \\sum_{x}^4 p(x) l(x) = \\frac{1}{2}\\times 2 + \\frac{1}{4}\\times 2 +2\\times \\frac{1}{8} \\times 2 = 2$$\n",
    "    \n",
    "\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecd8fe3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\", text-align:center>\n",
    "    \n",
    "    \n",
    "- <u>strategy 2</u>.  In order to get higher compression it makes sense to adscribe less bits to the letters that appear more frequently\n",
    "<br>\n",
    "<br>    \n",
    "$$\n",
    "A= 0~,~~ B=10~,~~ C=110~,~~ D=111 \\, .\n",
    "$$\n",
    "<br>  Average codeword length (number of bits)  per letter is lower now than in the uniform case\n",
    "<br>    \n",
    "<br>    \n",
    "$$L =   \\frac{1}{2}\\times 1 + \\frac{1}{4}\\times 2 +2\\times \\frac{1}{8} \\times 3 = \\frac{7}{4} < 2$$\n",
    "<br>\n",
    "    \n",
    "<b> Remarkably</b>, in the second case, the average length per letter <u>coincides with Shannon</u> entropy of the random variable\n",
    "<br>    \n",
    "$$\n",
    "H(X) = -\\frac{1}{2}\\log(1/2) - \\frac{1}{4}\\log(1/4) - 2\\times \\frac{1}{8}\\log(1/8) = \\frac{7}{4}\\, .\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988dae6e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Shannon optimal  compression theorem\n",
    "\n",
    "What is the **optimal coding** for a random variable $(X, p_X)$ with $N$ letters $a_1,..., a_N$?\n",
    "\n",
    "\n",
    "Shannon's brilliant idea is to shift the focus from coding individual letters to coding full sequences\n",
    "<br>\n",
    "<br>\n",
    "$$\\xn = x_1 x_2.... x_n$$\n",
    "<br>\n",
    "of $n$ letters pulled from $X$  independently from one another and *iid* (identically distributed).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8482e5d9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- strings $\\xn$ are elements of a new random variable $\\Xn$\n",
    "<br>\n",
    "\n",
    "- therefore the probability to draw a particular string is\n",
    "\n",
    "$$\n",
    "p_{{\\Xn}}(\\xn) = p_X(x_1) p_X(x_2) ...p_X(x_n) = \\prod_{i=1}^n p_X(x_i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faacb0c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- in the string $\\xn$ the letter $a_i$ appears  $N(a_i | \\xn)$ times. \n",
    "\n",
    "$$\n",
    "\\xn = \\sigma_n(\\underbrace{a_1\\cdots a_1}_{N(a_1|\\xn)}\\underbrace{a_2\\cdots a_2}_{N(a_2|\\xn)} \\cdots \\underbrace{a_N\\cdots a_N}_{N(a_N|\\xn)})  \n",
    "$$\n",
    "<br>\n",
    "Assuming $x_i$ are i.i.d. therefore we can rewrite $\\xn\\in X^n$\n",
    "\n",
    "$$\n",
    "p_{{\\Xn}}(\\xn)= \\prod_{i=1}^n p_X(x_i) = \\prod_{i=1}^N p_X(a_i)^{N(a_i,\\xn)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd61ae2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The *amazing fact* is that in the limit of large $n\\gg 1$, **not every possible sequence will appear**, but just those in a *very restricted set*, called the **typical set** $T(n,\\epsilon)$. \n",
    "\n",
    "Loosely speaking, $ \\bxn \\in T(n,\\epsilon)$ is composed mostly of those sequences for which \n",
    "\n",
    "$$ \\left|\\frac{N(a_i|  \\bxn)}{n}  - p_X(a_i) \\right| \\approx  \\epsilon \\ll 1$$\n",
    "\n",
    "with equality in the limit of long sequences $n\\to \\infty$. $~$\n",
    "\n",
    "Let us try to be more precise. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3bc98d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Define the  **typical set** $T(n,\\epsilon)$ by computing the **statistical average surprise**  of a string \n",
    "$\\xn = \\sigma_n( a_1\\cdots a_1 a_2\\cdots a_2... a_N\\cdots a_N )$\n",
    "\n",
    "\\begin{align}\n",
    "E(-\\log p_X(\\xn)) &= \\sum_{i=1}^N -\\frac{N(a_i|\\Xn)}{n} \\, \\log  p_X(a_i) \\\\\n",
    "&=  -\\frac{1}{n} \\sum_{i=1}^N \\log\\left( p_X(a_i)^{N(a_i|\\Xn)} \\right) \\\\\n",
    "&= -\\frac{1}{n} \\log\\left( \\prod_{i=1}^N p_X(a_i)^{N(a_i|\\Xn)}\\right) \\\\\n",
    "&=  -\\frac{1}{n} \\log \\, p_{{\\Xn}}(\\xn)  = \\frac{1}{n} i_{{\\Xn}}(\\xn) \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "If we compare this to the Shannon entropy of the random variable\n",
    "\n",
    "$$\n",
    "H(X) = -\\sum_i p_X(a_i) \\log p_X(a_i)\n",
    "$$\n",
    "we see clear the motivation to define \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4187bc4b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\", text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Definition:</b>  \n",
    "<br>\n",
    "The typical set $T(n,\\epsilon)$ is composed of all the strings $\\bar\\xn$ that fulfill\n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    " \\left\\vert\\frac{1}{n} i_{\\Xn}(\\bar\\xn) - H(X) \\right\\vert\\leq \\epsilon \\label{eqgrn}\n",
    "$$\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "or, in other words\n",
    "$$\n",
    "  H(X) - \\epsilon  ~\\leq ~  \\frac{1}{n}i_{\\Xn}\\big(\\bar\\xn) ~\\leq ~    H(X) +  \\epsilon      \\label{seqtip}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e3bc01",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\", text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Theorem:</b><i>$~$(Weak Law of Large Numbers</i>)  \n",
    "<br>\n",
    "Given any $\\epsilon, \\delta$, there exists a (large enough) value of $n$ for which the probability of $\\xn$ pertaining to $T(n,\\epsilon)$ is above $(1-\\delta)$\n",
    "<br> \n",
    "$$\n",
    "p_{\\Xn}(\\xn) \\in T(n,\\epsilon) \\geq (1-\\delta)\n",
    "$$\n",
    "<br>    \n",
    "</div>\n",
    "\n",
    "Hence, we can take $\\delta, \\epsilon\\to 0$ in the large $n$ limit, where only typical elements of $\\Xn$ will be generated. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d09144",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Define the **limiting typical set**  $~T = \\lim_{\\epsilon\\to 0} \\lim_{n\\to\\infty} T(n,\\epsilon)$,\n",
    "with cardinality $T$. The previous results can be summarized as follows \n",
    "\n",
    "\n",
    "- if $\\xn \\notin T(n,\\epsilon)$, the probability of appearance  will uniformly tend to zero $p_{\\Xn} (\\xn) \\stackrel{n\\to \\infty}{\\longrightarrow} 0$\n",
    "<br>\n",
    "\n",
    "- if $\\xn = \\bxn \\in T(n,\\epsilon)$ the probability of appearance  will uniformly tend to  $p_{\\Xn} (\\bxn) \\stackrel{n\\to\\infty}{\\longrightarrow} 1/|T|$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fce320",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Exerise 1.1</b> <i>(explain it to your computer)</i>\n",
    "<br>\n",
    "Consider a 5 symbol alphabet $X$ with probability $p_X = \\{0.01, 0.01, 0.01, 0.15, 0.82\\}$.\n",
    "Generate sets of strings of different sizes, $n$, and compute the average surprise. Plot a histogram that shows how this magnitude fits into an interval $H-\\epsilon$ and $H+\\epsilon$ as $n$ increases.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a8a89",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Depending on the distribution $p_X$, this can yield  <u>a dramatic reduction in the amount of possible sequences</u>. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb91f7b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For large $n$, the different $\\bxn$ will be just reorderings of a canonical sequence with $N(a_i|\\bar \\xn) = n p_X(a_i)$. Hence   \n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "N_{typical} = |T| = [\\bxn ]\\approx  \\frac{n!}{\\prod_{i=1}^N N(a_i | \\bxn)!} &= \\frac{n!}{\\prod_{i=1}^N (n p_X(a_i))!} = 2^{\\log_2 n! - \\sum_{i=1}^N \\log_2 (n p_X(a_i))!} \\\\ \n",
    "\\rule{0mm}{10mm}&\\sim  2^{n\\log_2 n - {n/\\log_2} e \\, -\\,  \\sum_i \\left( np_X(a_i) \\log n p_X(a_i)- n p_X(a_i) \\log_2 e \\right)}  \\\\ \n",
    "\\rule{0mm}{10mm}\n",
    "& =  2^{n H(X)}   \n",
    "\\rule{0mm}{8mm}   \\,  \\nonumber\n",
    "\\end{align}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c309e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Its size is *exponentially smaller* than that of possible sequences\n",
    "\n",
    "$$\n",
    "\\frac{N_{typical} }{N_{possible}} =  \\frac{[\\bxn ]}{N^n} = \\frac{2^{n H_X}}{  2^{n\\log N}} = 2^{-n(\\log N- H(X))}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465dad23",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\", text-align:center>\n",
    "<p style=\"text-align: left \">  \n",
    "<b> Summary </b>\n",
    "In the limit $n \\to \\infty$, the following three things happen\n",
    "\n",
    "- The number of sequences in the typical set is $|T| ~ \\stackrel{n\\to\\infty}{\\longrightarrow} ~ 2^{n H(X)}$.\n",
    "\n",
    "- The sequences in the typical set are equiprobable. Necessarily:\n",
    "\n",
    "$$\n",
    "  p(\\bxn_0) \\stackrel{n\\to\\infty}{\\longrightarrow} |T|^{-1} =  2^{- nH(X)}\n",
    "  $$\n",
    "\n",
    "- Any sequence <b>not</b> in the typical set has, in the limit, zero probability of occurring.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b87d5d1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The previous observations allow us to establish a message encoding procedure: it only makes sense to encode the $|T| = 2^{nH(X)}$ typical messages that are likely to appear.\n",
    "\n",
    "This can be done using strings with $n H(X)$ bits per message, or $H(X)$ bits per letter.\n",
    "\n",
    "If $X$ is a uniformly distributed alphabet, then $H(X) = \\log N$, and it is not possible to compress it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c53353",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "**Compression strategy**: \n",
    "\n",
    "- code by labelling only typical sequences $\\xn$ using bits.  \n",
    "<br>\n",
    "\n",
    "- Being there $|T| = 2^{nH(X)}  \\Rightarrow$ sequences,  coding them needs only  $nH(X)$ bits.\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081f1b7a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\", text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Theorem:</b> <i>(Shannon Noiseless Coding Theorem, (1984))</i>  \n",
    "<br>\n",
    "<br>    \n",
    "Let $X = \\{a_i, p(a_i)\\}$ be a stochastic source of $N$ letters. Words of length $n \\to \\infty$ admit a  <b>lossless encoding</b> thay makes use, on average, of $n H(X)$ bits\n",
    "<br>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c1fa8b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\", text-align:center>\n",
    "<p style=\"text-align: left \"></p>  \n",
    "    <b>Note:</b> \n",
    "    \n",
    "- the quantity $~ \\log N - H(X) ~$ quantifies the <i>compressibility</i> of a random source $X$   \n",
    "<br>\n",
    "    \n",
    "- By abuse of language, we may say that the optimal lossless encoding makes use, on average, of $H(X)$ *bits per letter* \n",
    "<br>\n",
    "\n",
    "- The equipartitioned alphabet, $p_1 = p_2 = ...=  1/N$,  has maximal entropy $H(X) = \\log N$, and each letter removes in average the highest amount of information. \n",
    "<br>\n",
    "\n",
    "- Therefore, it **admits no compression**\n",
    "\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a71374",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Landauer's principle\n",
    "\n",
    "This is another way to grasp the *physical entity* of information. It is based on the observation that the irreversible character of classical computation can be framed in the same was as we do with thermodynamics. \n",
    "\n",
    "It  therefore sets a lower bound to the amount of energy consumption in terms of heat disipation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42020b1a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\", text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Landauer's principle :</b> <i>(Rolf Landauer 1961)</i>  \n",
    "<br>\n",
    "There is a minimum amount of energy needed to erase one bit of information, and this is set by the surrounding temperature as given by\n",
    "<br>\n",
    "<br>    \n",
    "$$\n",
    "E \\geq  k_B T \\ln 2\n",
    "$$\n",
    "</p>    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fcdeea",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- At room temperature, the Landauer limit represents an energy of approximately 0.018 eV (2.9×10−21 J). \n",
    "<br>\n",
    "\n",
    "- As of 2012, modern computers use about a billion times as much energy per operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f996810",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='joint_ent'></a>\n",
    "# Joint Entropies\n",
    "[<<<](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f525326",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In a communication processes we alway deal with (at least) *two random variables*: the emission $X = (x, p(x))$ and the reception $Y = (y, p(y))$. \n",
    "\n",
    "The joint occurrence of a pair of values $x$ and $y$ at the ends of the channel defines a new random variable $XY = \\{xy, p(x,y)\\}$, where $p(x,y)$ is the *joint probability*.\n",
    "\n",
    "As with any random variable, we can also associate an entropy to it:\n",
    "\n",
    "$$\n",
    "H(X,Y) = - \\sum_{xy} p(x,y) \\log p(x,y)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baa7037",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One may wonder what is the contribution of $H(X)$ and $H(Y)$ to $H(X,Y)$. A very important property of the Shannon entropy of a joint probability is called: **subadditivity**. \n",
    "\n",
    "<a id='subadditivity'></a>\n",
    "<div class=\"alert alert-block alert-info\", text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Theorem:</b> <i>(subadditivity)</i>:  \n",
    " Given two random variables $X$ and $Y$ \n",
    "<br>\n",
    "<br>    \n",
    "$$ H(X,Y) \\leq H(X) + H(Y) $$\n",
    "<br>    \n",
    "</p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a249478f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A [proof of the subadditivity](#proof_of_subaditivity) will provided below based on the positivity of the Relative Entropy. \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "The subadditivity inequality is due to the possibility of correlations between $X$ and $Y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63da8fe8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='subadditivity'></a>\n",
    "<div class=\"alert alert-block alert-info\", text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Lemma:</b>    \n",
    "if $X$ and $Y$ are independent random variables, $\\Rightarrow p(x,y) = p(x) p(y)$ the subadditivity saturates\n",
    "<br>\n",
    "<br>    \n",
    "$$ H(X,Y) = H(X) + H(Y) $$\n",
    "<br>    \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "<details>\n",
    "<summary><p style=\"text-align:right\"> >><i>Proof</i> </p></summary>\n",
    "<br>    \n",
    "<br>\n",
    "\\begin{align}\n",
    "H(X,Y) &=  - \\sum_{xy} p(x)p(y) (\\log p(x) + \\log p(y)) =   - \\sum_{x} p(x)\\log p(x)\\sum_y p(y)  +\\sum_x p(x)\\sum_y p(y) \\log p(y)   \\\\  \\rule{0mm}{4mm}\n",
    "& =   - \\sum_{x} p(x)\\log p(x) - \\sum_y p(y) \\log p(y) = H(X) + H(Y)\n",
    "\\end{align}\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a81cf3a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='cond_ent'></a>\n",
    "## Conditional Entropy\n",
    "\n",
    "The random variable  $X|y = \\{x, p(x|y)\\}$ is the mathematical tool that **characterizes noise** in a communication channel.  \n",
    "\n",
    "Specifically, \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a25691",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $Q_{xy} = p(x|y)$ is the *conditional probability* that $x$ happens knowing that $y$ has occurred\n",
    "<br>\n",
    "\n",
    "- in *communication contexts*, it is the probability that,  $x$ is understood at  one end provided  $y$ was sent at other \n",
    "<br>\n",
    "\n",
    "- A **noiseless channel** would satisfy $Q_{xy} = \\delta_{xy}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853b56e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "As with any random variable, we can assign a Shannon entropy\n",
    "\n",
    "$$\n",
    "H(X|y) = -\\sum_x p(x|y)\\log(p(x|y)).\n",
    "$$\n",
    "\n",
    "Notice that $y$ is a *fixed event*. We can now average $H(X|y)$ over all possible events $y$, and obtain the **conditional entropy**:\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$\n",
    "H(X|Y) = \\sum_y p(y)H(X|y) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6251b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\", text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Lemma:</b> The conditional entropy equals \n",
    "<br>\n",
    "<br>    \n",
    "$$\n",
    "H(X|Y) =  H(X,Y) - H(Y)\n",
    "$$\n",
    "</div>\n",
    "\n",
    "<details>\n",
    "<summary><p style=\"text-align: right\"> >> Proof </p></summary>\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\sum_y p(y)H(X|y) &=& -\\sum_{yx} p(y)p(x|y)\\log(p(x|y)) \\\\\n",
    "   &=& -\\sum_{yx} p(x,y)\\log\\left(\\frac{p(x,y)}{p(y)}\\right) \\\\\n",
    "   &=& -\\sum_{yx} p(x,y)\\log(p(x,y)) + \\sum_{yx} p(x,y)\\log(p(y)) \\\\\n",
    "   &=& H(X,Y) + \\sum_{y} p(y)\\log(p(y)) \\\\\n",
    "   &=& H(X,Y) - H(Y)\n",
    "\\end{eqnarray}\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bf0499",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$H(X,Y)$ reflects the *residual uncertainty* in $X$ (*residual information*) after having received many messages $y \\in Y$.\n",
    "\n",
    "- If $X$ and $Y$ are **independent** variables, then $H(X|Y) = H(X)+ H(Y) - H(Y) = H(X)$, meaning that knowing $Y$ does not reduce the uncertainty removed by learning $X$  \n",
    "<br>\n",
    "\n",
    "- If $X = Y$, are **correlated** then $H(X|Y) = 0$, knowing $Y$ removes all the uncertainty in $X$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671d4f38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='mut_inf'></a>\n",
    "## Mutual Information\n",
    "\n",
    "\n",
    " - $H(X)$ is a measure of the amount of information $X$—that is, the average  amount of uncertainty that is removed when an event $x$ is known.\n",
    "<br>\n",
    "\n",
    " - $H(X|Y)$ is a measure of the information in $X$ after  knowing $Y$\n",
    "\n",
    "Necessarily, $H(X|Y) \\leq  H(X)$, since knowing $y$ can only reduce uncertainty in $X$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a3c2ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The difference is therefore positive, $H(X) - H(X|Y) > 0$, and it quantifies the amount of information *gained* by the receiver. This is called *mutual information*:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\", text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Definition:</b> The <b>mutual information</b> of two random variables $I(X,Y)$ equals\n",
    "<br>\n",
    "<br>    \n",
    "$$\n",
    "I(X,Y) = H(X) + H(Y) - H(X,Y) \n",
    "$$\n",
    "</div>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc418344",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The mutual information is non-negative $I \\geq 0$. This trivially follows from the [subadditivity property](#subbaditivity) of the Shannon entropy of a joint random variable.\n",
    "<br>\n",
    "<br>\n",
    "- We see that the gain in information is a symmetric quantity under $X \\leftrightarrow Y$ \n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "I(X,Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) \\, .\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb34c7d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Therefore, $I(X,Y)$ is a measure of the *correlations* between the two variables.\n",
    "\n",
    "- If there is no correlation, then measuring $Y$ provides no information, and the mutual information is zero:  $I(X,Y) = 0$.  \n",
    "<br>\n",
    "\n",
    "- If they are perfectly correlated, $I(X,Y) = H(X) = H(Y)$, then there is a perfect correlation between emission and reception, learning $X$ is the same as learning $Y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeb3289",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='rel_ent'></a>\n",
    "\n",
    "## Relative Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed2ad38",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Relative entropy is a measure of the *distance* between the probability distributions $p(x)$ and $q(x)$, defined over the same sample space $x \\in X$  \n",
    "<br>\n",
    "\n",
    "\\begin{eqnarray}\n",
    "H(p|| q) &=& \\sum_x p(x) \\big( \\log p(x)- \\log q(x) \\big)  \\\\\n",
    "&=&  - H(X) -  \\sum_x p(x) \\log q(x)\\, .\n",
    "\\end{eqnarray}\n",
    "\n",
    "This distance is sometimes called the *Kullback-Leibler divergence*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e32a13e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\", text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Theorem:</b>  <i>(Gibbs' Inequality)</i>:  \n",
    "<br>\n",
    "Relative entropy is <i>non-negative</i>. That is, for two arbitrary distributions $p(x)$ and $q(x)$, we have:\n",
    "<br>\n",
    "<br>    \n",
    "$$\n",
    "H(p|| q) ~=~  \\sum_x p(x)  \\log p(x) -  \\sum_x p(x)  \\log q(x)  ~\\geq~ 0\n",
    "$$\n",
    "<br>\n",
    "and the inequality is saturated if and only if $p(x) = q(x)$, that is, the distributions are identical.\n",
    "</p>\n",
    "</div>    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc35af4f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<details>\n",
    "<summary><p style=\"text-align:right\"> >><i>Proof</i> </p></summary>\n",
    "<br>\n",
    "Assuming $p(x)\\neq 0 ~\\forall x$ we have:\n",
    "\\begin{eqnarray}\n",
    "H(p|| q) = \\sum_x p(x)  \\log \\frac{p(x)}{q(x)} & ~= ~&   \\sum_x p(x) \\log\\frac{p(x)}{q(x)}    ~= ~ - \\sum_x p(x)   \\log\\frac{q(x)}{p(x)} \\nonumber\\\\\n",
    "&~\\geq ~& -  \\sum_x p(x) \\left( \\frac{q(x)}{p(x)} -1\\right)   ~= ~ -   \\sum_x (q(x) - p(x))  \\nonumber\\\\\n",
    "&=&   0  \\nonumber\n",
    "\\end{eqnarray} \n",
    "<br>\n",
    "where the inequality follows from the fact that $\\log x \\leq (x-1)$.  \n",
    "For equality, $p(x) = q(x)$ is a sufficient condition. To see that it is also necessary, consult references.\n",
    "<br>\n",
    "<img src=\"figures/logarithm.png\" width=\"30%\" style=\"margin:auto\"/>\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457f5c48",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "By considering several particular cases, we can now prove some of the statements made earlier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fc93f4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Shannon entropy bound\n",
    "<i>Proof:</i>\n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "H(X) \\leq \\log N\n",
    "$$\n",
    "\n",
    "<details>\n",
    "<summary><p style=\"text-align:right\"> >><i>Proof</i> </p></summary>\n",
    "<br>\n",
    "<br>\n",
    "Just take  \n",
    "$q_X(x) = 1/N$. Then  \n",
    "$H(p_X || q_X) = - H(X) + \\log N \\geq 0$.\n",
    "<br>\n",
    "<br>\n",
    "The inequality is saturated when $X$ is also uniformly distributed, i.e., $p_X(x) = q_X(x) = 1/N ~\\Rightarrow~ H(p_X || q_X) = 0$.\n",
    "<br>\n",
    "<br>\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b6cd73",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='proof_of_subaditivity'></a>\n",
    "- Shannon entropy subadditivity\n",
    "\n",
    "$$\n",
    "H(X,Y) \\leq H(X) + H(Y)\n",
    "$$\n",
    "\n",
    "This proves also that the Mutual Information is positive.\n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "I(X,Y) = H(X) + H(Y) - H(X,Y) \\geq 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1c1765",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<details>\n",
    "<summary><p style=\"text-align:right\"> >><i>Proof</i> </p></summary>\n",
    "<i>Proof</i>\n",
    "<br>\n",
    "<br>\n",
    "Now consider the joint random variable $XY \\to \\{(x,y)\\}$, where $p_X$ is the joint distribution $p_{XY}$ and $q_X$ is the factorized distribution $p_X p_Y$, with  \n",
    "$p_X(x) = \\sum_y p_{XY}(x,y)$ and $p_Y(y) = \\sum_x p_{XY}(x,y)$ being the marginal distributions.  \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\\begin{eqnarray}\n",
    "H(p_{X,Y}(x, y) || p_X(x) p_Y(y)) \n",
    "&=& - H(X,Y) - \\sum_{x,y} p_{XY}(x,y) \\log p_X(x) - \\sum_{x,y} p_{XY}(x,y) \\log p_Y(y) \\nonumber\\\\\n",
    "&=& - H(X,Y) - \\sum_x p_X(x) \\log p_X(x) - \\sum_y p_Y(y) \\log p_Y(y) \\nonumber\\\\\n",
    "&=& - H(X,Y) + H(X) + H(Y) \\\\\n",
    "&=& I(X,Y) \\geq 0 \\rule{0mm}{8mm}\n",
    "\\end{eqnarray}\n",
    "<br>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce1fe11",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This inequality proves that mutual information is non-negative. Equivalently, the subadditivity\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$\n",
    "H(X,Y) \\leq H(X) + H(Y)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05569bc1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='proof_of_concavity'></a>\n",
    "\n",
    "- Shannon entropy concavity\n",
    "<br>\n",
    "Let $p_Z = \\lambda p_X + (1-\\lambda) p_Y$ be the $\\lambda-$averaged distribution of $p_X$ and $p_Y$ over the same set $X = Y = Z = \\{x\\}$. The concavity property states that, for any $\\lambda$ \n",
    "\n",
    "$$\n",
    "H(p_Z) \\geq \\lambda H(p_X) + (1-\\lambda) H(p_Y)\n",
    "$$\n",
    "\n",
    "<details>\n",
    "<summary><p style=\"text-align:right\"> >><i>Proof</i> </p></summary>\n",
    "<i>Proof:</i>  from its definition \n",
    "\\begin{align}\n",
    "H(p_X||p_Z) &= - H(p_X) - \\sum_x p_X(x) \\log p_Z(x) \\\\\n",
    "H(p_Y||p_Z) &= - H(p_Y) - \\sum_x p_Y(x) \\log p_Z(x) \\\\\n",
    "\\end{align}\n",
    "The positivity of the relative entropy implies that, for $0\\leq \\lambda \\leq 1$\n",
    "\n",
    "\\begin{align}\n",
    " 0 & \\leq \\lambda H(p_X||p_Z) + (1-\\lambda)H(p_Y||p_Z)   \\\\ \\rule{0mm}{8mm}\n",
    " & =  \\lambda \\left(- H(p_X) - \\sum_x p_X(x) \\log p_Z(x) \\right)  + (1-\\lambda) \\left( - H(p_Y) - \\sum_x p_Y(x) \\log p_Z(x)\\right) \\\\  \\rule{0mm}{4mm}\n",
    "& =  -\\lambda H(p_X) - (1-\\lambda) H(p_Y) - \\sum_x \\big(\\underbrace{\\lambda p_X(x) + (1-\\lambda)p_Y(x)}_{p_Z(x)}\\big)\\log p_Z(x) \\\\  \\rule{0mm}{4mm}\n",
    " & = -\\lambda H(p_X) - (1-\\lambda) H(p_Y) + H(p_Z)\n",
    "\\end{align}\n",
    "\n",
    "From here it follows the concavity property\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a861656",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bibliography of this chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719aa250",
   "metadata": {},
   "source": [
    "There are many excellent books on this subject  \n",
    "\n",
    "- For a straight reading I recommend chapters 11 and 12 of M.A. Nielsen and I.L. Chuang, *Quantum Information and Quantum Computation*, Cambridge. \n",
    "\n",
    "\n",
    "- For a further reading, the book of T. M. Cover and J.A. Thomas, *Elements of Information Theory*, Wiley.\n",
    "\n",
    "- For a very short summary with personal views, E. Witten https://arxiv.org/abs/1805.11965v4\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7c10f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
